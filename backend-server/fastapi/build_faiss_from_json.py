import logging
import faiss
import numpy as np
import json
from sentence_transformers import SentenceTransformer
import os, pickle
from tqdm import tqdm
import torch
import gc

# ì„¤ì •
CHUNK_SIZE = 1000
INDEX_SAVE_PATH = "faiss_store/index.faiss"
META_SAVE_PATH = "faiss_store/metadata.pkl"
LAST_PROCESSED_PATH = "faiss_store/last_processed.txt"
JSON_FILE_PATH = "recipes_fixed.json"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ëª¨ë¸ ë¡œë“œ
logger.info("ğŸ“¦ SentenceTransformer ëª¨ë¸ ë¡œë”© ì¤‘...")
model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS", device="cuda")
dimension = model.get_sentence_embedding_dimension()

# FAISS ì €ì¥ í´ë” ì´ˆê¸°í™”
if not os.path.exists(INDEX_SAVE_PATH) or not os.path.exists(META_SAVE_PATH):
    logger.warning("â— ê¸°ì¡´ index ë˜ëŠ” metadata íŒŒì¼ ì—†ìŒ â†’ ì²˜ìŒë¶€í„° ì‹œì‘")
    for path in [INDEX_SAVE_PATH, META_SAVE_PATH, LAST_PROCESSED_PATH]:
        if os.path.exists(path):
            os.remove(path)

try:
    # JSON íŒŒì¼ ë¡œë”©
    logger.info("ğŸ“„ JSON íŒŒì¼ ë¡œë”© ì¤‘...")
    with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    logger.info(f"âœ… ì´ {len(data)}ê°œ ë ˆì‹œí”¼ ë¡œë”© ì™„ë£Œ")

    # í…ìŠ¤íŠ¸ ë³€í™˜ í•¨ìˆ˜
    def recipe_to_text(recipe):
        ingredients = recipe.get('ingredients', '')
        return f"ì´ ìš”ë¦¬ì˜ ì¬ë£ŒëŠ” {ingredients}ì…ë‹ˆë‹¤."

    texts = [recipe_to_text(recipe) for recipe in data]

    # ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
    metadata = []

    # ì²˜ë¦¬ ì§€ì  ë¡œë“œ
    if os.path.exists(LAST_PROCESSED_PATH):
        with open(LAST_PROCESSED_PATH, "r") as f:
            last_processed = int(f.read().strip() or 0)
    else:
        last_processed = 0

    # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    logger.info("ğŸ“ ìƒˆë¡œìš´ FAISS ì¸ë±ìŠ¤ ìƒì„±")
    index = faiss.IndexFlatL2(dimension)

    # ë²¡í„°í™” ë° ì €ì¥ ë£¨í”„
    for start in range(last_processed, len(texts), CHUNK_SIZE):
        end = min(start + CHUNK_SIZE, len(texts))
        text_chunk = texts[start:end]

        filtered_texts = []
        filtered_ids = []

        for i, text in enumerate(text_chunk):
            if isinstance(text, str) and len(text.strip()) > 0:
                filtered_texts.append(text)
                recipe = data[start + i]
                filtered_ids.append({
                    "id": recipe.get("id", start + i),
                    "title": recipe.get("title", ""),
                    "ingredients": recipe.get("ingredients", ""),
                    "content": recipe.get("content", "")
                })

        logger.info(f"ğŸ§  ì„ë² ë”© ì¤‘: {start} ~ {end} (ì´ {len(filtered_texts)}ê°œ)")

        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()

            emb_chunk = model.encode(filtered_texts, show_progress_bar=True)

            if emb_chunk.ndim != 2 or emb_chunk.shape[1] != dimension:
                logger.error(f"âŒ ì˜ëª»ëœ ë²¡í„° ì°¨ì›: {emb_chunk.shape}")
                continue

            index.add(np.array(emb_chunk))
            metadata.extend(filtered_ids)

            # ì €ì¥
            os.makedirs(os.path.dirname(INDEX_SAVE_PATH), exist_ok=True)
            faiss.write_index(index, INDEX_SAVE_PATH)
            with open(META_SAVE_PATH, "wb") as f:
                pickle.dump(metadata, f)
            with open(LAST_PROCESSED_PATH, "w") as f:
                f.write(str(end))

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del emb_chunk
            gc.collect()

        except Exception as e:
            logger.exception(f"â— ì˜¤ë¥˜ ë°œìƒ: {start}-{end} êµ¬ê°„ â†’ {str(e)}")
            break

    logger.info("âœ… ì „ì²´ ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ!")

except Exception as e:
    logger.exception(f"â— ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    raise

finally:
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

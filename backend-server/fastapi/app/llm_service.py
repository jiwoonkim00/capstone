"""
Hugging Face LLM ì„œë¹„ìŠ¤
Hugging Faceì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ì œê³µ
"""

import os
import logging
from typing import Optional, List, Dict
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    pipeline
)
import torch

logger = logging.getLogger(__name__)

class HuggingFaceLLMService:
    """Hugging Face ëª¨ë¸ì„ ì‚¬ìš©í•œ LLM ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model_name = os.getenv(
            "HF_MODEL_NAME", 
            "00PJH/Llama-3.2-Korean-GGACHI-1B-Instruct-v1-koToEn"  # ê¸°ë³¸ê°’: Instruct ëª¨ë¸
        )
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.is_instruct_model = "instruct" in self.model_name.lower() or "llama-3.2" in self.model_name.lower()
        self._load_model()
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ (ì²˜ìŒ í˜¸ì¶œ ì‹œ í•œ ë²ˆë§Œ)"""
        try:
            logger.info(f"ğŸ¤– Hugging Face ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_name}")
            logger.info(f"ğŸ“± Device: {self.device}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # Pipeline ìƒì„±
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1,
                max_length=512,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            
            # Instruct ëª¨ë¸ìš© íŠ¹ìˆ˜ í† í° í™•ì¸
            if self.is_instruct_model:
                logger.info("ğŸ“ Instruct ëª¨ë¸ ê°ì§€: íŠ¹ìˆ˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©")
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {self.model_name}")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            raise
    
    def generate(
        self, 
        prompt: str, 
        max_length: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        **kwargs
    ) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            if not self.pipeline:
                raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # Instruct ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            if self.is_instruct_model:
                # Llama 3.2 Instruct í˜•ì‹ ì‚¬ìš©
                full_prompt = self._format_instruct_prompt(prompt)
            else:
                full_prompt = prompt
            
            # ìƒì„± ì‹¤í–‰
            result = self.pipeline(
                full_prompt,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                num_return_sequences=1,
                return_full_text=False,
                **kwargs
            )
            
            # ê²°ê³¼ ì¶”ì¶œ
            generated_text = result[0]["generated_text"].strip()
            
            # Instruct ëª¨ë¸ ì‘ë‹µ ì •ë¦¬ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œê±°)
            if self.is_instruct_model:
                generated_text = self._clean_instruct_response(generated_text)
            
            logger.info(f"ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(generated_text)})")
            return generated_text
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _format_instruct_prompt(self, prompt: str) -> str:
        """Instruct ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í˜•ì‹ ë³€í™˜ (í† í¬ë‚˜ì´ì €ì˜ ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš©)"""
        system_message = "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ ì‹œìŠ¤í…œ/ì‚¬ìš©ì ë©”ì‹œì§€ ë¶„ë¦¬
        if "[ì‹œìŠ¤í…œ]" in prompt:
            parts = prompt.split("[ì‚¬ìš©ì]")
            if len(parts) > 1:
                system_part = parts[0].replace("[ì‹œìŠ¤í…œ]", "").strip()
                user_message = parts[1].strip()
            else:
                user_message = prompt.replace("[ì‹œìŠ¤í…œ]", "").replace("[ì‚¬ìš©ì]", "").strip()
                system_part = system_message
        else:
            user_message = prompt.strip()
            system_part = system_message
        
        # í† í¬ë‚˜ì´ì €ì˜ apply_chat_template ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        if hasattr(self.tokenizer, 'apply_chat_template') and self.tokenizer.chat_template is not None:
            try:
                messages = [
                    {"role": "system", "content": system_part},
                    {"role": "user", "content": user_message}
                ]
                formatted = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                return formatted
            except Exception as e:
                logger.warning(f"apply_chat_template ì‹¤íŒ¨, ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©: {str(e)}")
        
        # Fallback: ê¸°ë³¸ Llama 3.2 Instruct í˜•ì‹
        formatted = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_part}<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        return formatted
    
    def _clean_instruct_response(self, response: str) -> str:
        """Instruct ëª¨ë¸ ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°"""
        # Llama íŠ¹ìˆ˜ í† í° ì œê±°
        response = response.replace("<|eot_id|>", "").strip()
        response = response.replace("<|start_header_id|>", "").strip()
        response = response.replace("<|end_header_id|>", "").strip()
        
        # í—¤ë” íƒœê·¸ ì œê±°
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        
        return response
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        max_length: int = 256,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            # ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
            prompt = self._format_messages(messages)
            
            return self.generate(
                prompt=prompt,
                max_length=max_length,
                temperature=temperature,
                **kwargs
            )
            
        except Exception as e:
            logger.error(f"ì±„íŒ… ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _format_messages(self, messages: List[Dict[str, str]]) -> str:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜"""
        if self.is_instruct_model:
            # Instruct ëª¨ë¸ìš©: í† í¬ë‚˜ì´ì €ì˜ apply_chat_template ì‚¬ìš©
            if hasattr(self.tokenizer, 'apply_chat_template') and self.tokenizer.chat_template is not None:
                try:
                    # ë©”ì‹œì§€ í˜•ì‹ ì •ê·œí™”
                    formatted_messages = []
                    for msg in messages:
                        role = msg.get("role", "user")
                        content = msg.get("content", "")
                        if role in ["system", "user", "assistant"]:
                            formatted_messages.append({"role": role, "content": content})
                    
                    if formatted_messages:
                        formatted = self.tokenizer.apply_chat_template(
                            formatted_messages,
                            tokenize=False,
                            add_generation_prompt=True
                        )
                        return formatted
                except Exception as e:
                    logger.warning(f"apply_chat_template ì‹¤íŒ¨, ìˆ˜ë™ ë³€í™˜ ì‚¬ìš©: {str(e)}")
            
            # Fallback: ìˆ˜ë™ ë³€í™˜
            system_msg = None
            user_msgs = []
            
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "system":
                    system_msg = content
                elif role == "user":
                    user_msgs.append(content)
            
            user_message = user_msgs[-1] if user_msgs else messages[-1].get("content", "") if messages else ""
            system_message = system_msg or "ë„ˆëŠ” ì¹œì ˆí•˜ê³  ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            
            return self._format_instruct_prompt(
                f"[ì‹œìŠ¤í…œ]\n{system_message}\n[ì‚¬ìš©ì]\n{user_message}"
            )
        else:
            # ì¼ë°˜ ëª¨ë¸ìš© í˜•ì‹
            prompt_parts = []
            
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "system":
                    prompt_parts.append(f"[ì‹œìŠ¤í…œ]\n{content}\n")
                elif role == "user":
                    prompt_parts.append(f"[ì‚¬ìš©ì]\n{content}\n")
                elif role == "assistant":
                    prompt_parts.append(f"[ì–´ì‹œìŠ¤í„´íŠ¸]\n{content}\n")
            
            # ë§ˆì§€ë§‰ì— ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì‹œì‘ í‘œì‹œ
            prompt_parts.append("[ì–´ì‹œìŠ¤í„´íŠ¸]\n")
            
            return "\n".join(prompt_parts)
    
    def is_loaded(self) -> bool:
        """ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.model is not None and self.tokenizer is not None


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_llm_service: Optional[HuggingFaceLLMService] = None

def get_llm_service() -> HuggingFaceLLMService:
    """LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
    global _llm_service
    if _llm_service is None:
        _llm_service = HuggingFaceLLMService()
    return _llm_service

